{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhkid/GDG-DevFest-Codelab-24/blob/main/problems/02-a-FGSM-Adversarial-Attack_fill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3d118b903e62e7c1"
      },
      "cell_type": "markdown",
      "source": [
        "# Fast Gradient Sign Method (FGSM) Adversarial Attack Workshop\n",
        "\n",
        "## Introduction to Adversarial Attacks\n",
        "In this workshop, we'll explore how to create adversarial examples using the Fast Gradient Sign Method (FGSM). These examples are carefully crafted perturbations that can cause a deep learning model to misclassify images, despite the changes being nearly imperceptible to human eyes.\n",
        "\n",
        "## Setup and Dependencies"
      ],
      "id": "3d118b903e62e7c1"
    },
    {
      "metadata": {
        "id": "21040ea175aca36e"
      },
      "cell_type": "markdown",
      "source": [
        "### Install and Import Dependencies\n",
        "Run this cell to install and import all required libraries"
      ],
      "id": "21040ea175aca36e"
    },
    {
      "metadata": {
        "id": "initial_id"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "id": "initial_id"
    },
    {
      "metadata": {
        "id": "787b5913a4cf9c02"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Pre-trained Model"
      ],
      "id": "787b5913a4cf9c02"
    },
    {
      "metadata": {
        "id": "cbfe455f2002e04c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load Pre-trained ResNet50 Model\n",
        "# This cell loads a pre-trained ResNet50 model that we'll try to fool\n",
        "\n",
        "model = ResNet50(weights='imagenet')\n",
        "print(\"Model loaded successfully!\")"
      ],
      "id": "cbfe455f2002e04c"
    },
    {
      "metadata": {
        "id": "bcb930881fb03f9e"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "id": "bcb930881fb03f9e"
    },
    {
      "metadata": {
        "id": "bf53447403723eaf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Image Preprocessing Function\n",
        "# This function prepares images for our model\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image for ResNet50.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Preprocessed image array\n",
        "    \"\"\"\n",
        "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = preprocess_input(image)\n",
        "    return image"
      ],
      "id": "bf53447403723eaf"
    },
    {
      "metadata": {
        "id": "124da221fe10b66d"
      },
      "cell_type": "markdown",
      "source": [
        "## FGSM Attack Implementation"
      ],
      "id": "124da221fe10b66d"
    },
    {
      "metadata": {
        "id": "c1c348949d0bffdc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# This cell contains the core FGSM attack implementation\n",
        "\n",
        "def fgsm_attack(image, epsilon, data_grad):\n",
        "    \"\"\"\n",
        "    Implements the Fast Gradient Sign Method attack.\n",
        "\n",
        "    Args:\n",
        "        image: Input image\n",
        "        epsilon: Attack strength parameter\n",
        "        data_grad: Gradient of the loss with respect to the input image\n",
        "\n",
        "    Returns:\n",
        "        Adversarial image\n",
        "    \"\"\"\n",
        "    sign_data_grad = tf.sign(data_grad)\n",
        "    # TODO: Fill in the appropriate code\n",
        "    perturbed_image = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "    perturbed_image = tf.clip_by_value(perturbed_image, -1, 1)\n",
        "    return perturbed_image\n",
        "\n",
        "def generate_adversarial_example(image, epsilon):\n",
        "    \"\"\"\n",
        "    Generates an adversarial example using FGSM.\n",
        "\n",
        "    Args:\n",
        "        image: Input image\n",
        "        epsilon: Attack strength parameter\n",
        "\n",
        "    Returns:\n",
        "        Adversarial version of the input image\n",
        "    \"\"\"\n",
        "    # TODO: Fill in the appropriate code\n",
        "    image_tensor = tf.convert_to_tensor(\"\"\"TODO: Fill in the appropriate code\"\"\")\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image_tensor)\n",
        "        # TODO: Fill in the appropriate code\n",
        "        prediction = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "        loss = tf.keras.losses.categorical_crossentropy(prediction, prediction)\n",
        "\n",
        "    gradient = tape.gradient(loss, image_tensor)\n",
        "    # TODO: Fill in the appropriate code\n",
        "    perturbed_image = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "    return perturbed_image"
      ],
      "id": "c1c348949d0bffdc"
    },
    {
      "metadata": {
        "id": "656d6b360aa73df9"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate and Test Adversarial Example"
      ],
      "id": "656d6b360aa73df9"
    },
    {
      "metadata": {
        "id": "4e7b382530f2bf04"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Run this cell to create and test an adversarial example\n",
        "# You can modify the epsilon value to control attack strength\n",
        "\n",
        "# TODO: Fill in the appropriate code\n",
        "image_path = \"\"\"TODO: Fill in the appropriate code\"\"\"  # @param {type:\"string\"}\n",
        "epsilon = 0.089  # @param {type:\"slider\", min:0.001, max:0.1, step:0.001}\n",
        "\n",
        "# Load and preprocess the image\n",
        "# TODO: Fill in the appropriate code\n",
        "original_image = preprocess_image(\"\"\"TODO: Fill in the appropriate code\"\"\")\n",
        "\n",
        "# Generate adversarial example\n",
        "# TODO: Fill in the appropriate code\n",
        "adversarial_image = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "\n",
        "# Make predictions\n",
        "# TODO: Fill in the appropriate code\n",
        "original_pred = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "adversarial_pred = \"\"\"TODO: Fill in the appropriate code\"\"\"\n",
        "\n",
        "# Decode predictions\n",
        "original_label = decode_predictions(original_pred)[0][0]\n",
        "# TODO: Fill in the appropriate code\n",
        "adversarial_label = \"\"\"TODO: Fill in the appropriate code\"\"\""
      ],
      "id": "4e7b382530f2bf04"
    },
    {
      "metadata": {
        "id": "ed9efa4441f24a70"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize Results"
      ],
      "id": "ed9efa4441f24a70"
    },
    {
      "metadata": {
        "id": "f9069837d162ca04"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Visualize Original vs Adversarial Images\n",
        "# This cell will display the original and adversarial images side by side\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(tf.keras.preprocessing.image.array_to_img(original_image[0]))\n",
        "plt.title(f\"Original: {original_label[1]}\\nConfidence: {original_label[2]:.2f}\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(tf.keras.preprocessing.image.array_to_img(adversarial_image[0].numpy()))\n",
        "plt.title(f\"Adversarial: {adversarial_label[1]}\\nConfidence: {adversarial_label[2]:.2f}\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Original prediction: {original_label[1]} ({original_label[2]:.2f})\")\n",
        "print(f\"Adversarial prediction: {adversarial_label[1]} ({adversarial_label[2]:.2f})\")"
      ],
      "id": "f9069837d162ca04"
    },
    {
      "metadata": {
        "id": "a5e513acad49e986"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra Exercise Section\n",
        "Try experimenting with:\n",
        "1. Different epsilon values - how does this affect the attack's success and visibility?\n",
        "2. Different input images - do some types of images work better than others?\n",
        "3. Different target classes - can you modify the attack to target a specific class?"
      ],
      "id": "a5e513acad49e986"
    },
    {
      "metadata": {
        "id": "9e2c31c68a04b3a0"
      },
      "cell_type": "markdown",
      "source": [
        "## Additional Notes:\n",
        "- The epsilon value controls the strength of the attack. Larger values create stronger attacks but more visible perturbations.\n",
        "- Some images may be more resistant to adversarial attacks than others.\n",
        "- The success of the attack can vary depending on the confidence of the original prediction."
      ],
      "id": "9e2c31c68a04b3a0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}